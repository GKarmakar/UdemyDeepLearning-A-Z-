{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "movies = pd.read_csv(\"./data/ml-1m/movies.dat\", sep = \"::\", header=None, engine='python', encoding='latin-1')\n",
    "users = pd.read_csv(\"./data/ml-1m/users.dat\", sep = \"::\", header=None, engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv(\"./data/ml-1m/ratings.dat\", sep = \"::\", header=None, engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                   1                             2\n",
       "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4  5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3      4\n",
       "0  1  F   1  10  48067\n",
       "1  2  M  56  16  70072\n",
       "2  3  M  25  15  55117\n",
       "3  4  M  45   7  02460\n",
       "4  5  M  25  20  55455"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare training and test set\n",
    "train_set = pd.read_csv(\"./data/ml-100k/u1.base\", delimiter='\\t')\n",
    "train_set = np.array(train_set, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,         2,         3, 876893171],\n",
       "       [        1,         3,         4, 878542960],\n",
       "       [        1,         4,         3, 876893119],\n",
       "       [        1,         5,         3, 889751712],\n",
       "       [        1,         7,         4, 875071561]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,        10,         3, 875693118],\n",
       "       [        1,        12,         5, 878542960],\n",
       "       [        1,        14,         5, 874965706],\n",
       "       [        1,        17,         3, 875073198],\n",
       "       [        1,        20,         4, 887431883]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv(\"./data/ml-100k/u1.test\", delimiter='\\t')\n",
    "test_set = np.array(test_set, dtype='int')\n",
    "test_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#total user and movies\n",
    "nb_users = int(max(max(train_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(train_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert data such that users are in rows and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users+1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = convert(train_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert data into torch tensors\n",
    "train_set = torch.FloatTensor(train_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a newral network architecture - Autoencoder\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE,self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x)) # First encoding from input\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sae = SAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1Loss: 1.76632954092\n",
      "epoch: 2Loss: 1.09639336454\n",
      "epoch: 3Loss: 1.05349361746\n",
      "epoch: 4Loss: 1.03828138331\n",
      "epoch: 5Loss: 1.03097391166\n",
      "epoch: 6Loss: 1.02648157503\n",
      "epoch: 7Loss: 1.02391810081\n",
      "epoch: 8Loss: 1.02181320818\n",
      "epoch: 9Loss: 1.02071473789\n",
      "epoch: 10Loss: 1.0199012669\n",
      "epoch: 11Loss: 1.01893917302\n",
      "epoch: 12Loss: 1.01829102679\n",
      "epoch: 13Loss: 1.01787672019\n",
      "epoch: 14Loss: 1.01752504983\n",
      "epoch: 15Loss: 1.01719343309\n",
      "epoch: 16Loss: 1.01694731622\n",
      "epoch: 17Loss: 1.01679829112\n",
      "epoch: 18Loss: 1.01641970477\n",
      "epoch: 19Loss: 1.0163903684\n",
      "epoch: 20Loss: 1.01628627154\n",
      "epoch: 21Loss: 1.01602892246\n",
      "epoch: 22Loss: 1.01589279739\n",
      "epoch: 23Loss: 1.0160285077\n",
      "epoch: 24Loss: 1.01592044782\n",
      "epoch: 25Loss: 1.01580162347\n",
      "epoch: 26Loss: 1.01551361134\n",
      "epoch: 27Loss: 1.01517431046\n",
      "epoch: 28Loss: 1.01509100619\n",
      "epoch: 29Loss: 1.01302391856\n",
      "epoch: 30Loss: 1.0119296137\n",
      "epoch: 31Loss: 1.00996476006\n",
      "epoch: 32Loss: 1.00935208517\n",
      "epoch: 33Loss: 1.00579951506\n",
      "epoch: 34Loss: 1.00574159027\n",
      "epoch: 35Loss: 1.00166381501\n",
      "epoch: 36Loss: 1.00080692845\n",
      "epoch: 37Loss: 0.99614192399\n",
      "epoch: 38Loss: 0.997538750601\n",
      "epoch: 39Loss: 0.992803460025\n",
      "epoch: 40Loss: 0.992494162227\n",
      "epoch: 41Loss: 0.987709987325\n",
      "epoch: 42Loss: 0.988706063041\n",
      "epoch: 43Loss: 0.987820441652\n",
      "epoch: 44Loss: 0.9847814772\n",
      "epoch: 45Loss: 0.982581071513\n",
      "epoch: 46Loss: 0.985838178522\n",
      "epoch: 47Loss: 0.984309157697\n",
      "epoch: 48Loss: 0.97924694269\n",
      "epoch: 49Loss: 0.976891173046\n",
      "epoch: 50Loss: 0.982238789736\n",
      "epoch: 51Loss: 0.977060877046\n",
      "epoch: 52Loss: 0.975693840548\n",
      "epoch: 53Loss: 0.972183064438\n",
      "epoch: 54Loss: 0.971762101256\n",
      "epoch: 55Loss: 0.968777316827\n",
      "epoch: 56Loss: 0.969664972583\n",
      "epoch: 57Loss: 0.96747371224\n",
      "epoch: 58Loss: 0.965155458183\n",
      "epoch: 59Loss: 0.963305992024\n",
      "epoch: 60Loss: 0.963989724156\n",
      "epoch: 61Loss: 0.958610209965\n",
      "epoch: 62Loss: 0.958549549836\n",
      "epoch: 63Loss: 0.956951805359\n",
      "epoch: 64Loss: 0.954229806758\n",
      "epoch: 65Loss: 0.951625362689\n",
      "epoch: 66Loss: 0.952188093893\n",
      "epoch: 67Loss: 0.951316530114\n",
      "epoch: 68Loss: 0.952728248129\n",
      "epoch: 69Loss: 0.950332559483\n",
      "epoch: 70Loss: 0.951245343095\n",
      "epoch: 71Loss: 0.946621895674\n",
      "epoch: 72Loss: 0.948175943089\n",
      "epoch: 73Loss: 0.946862668124\n",
      "epoch: 74Loss: 0.949907707937\n",
      "epoch: 75Loss: 0.946898500104\n",
      "epoch: 76Loss: 0.946237418131\n",
      "epoch: 77Loss: 0.946090749721\n",
      "epoch: 78Loss: 0.94585861251\n",
      "epoch: 79Loss: 0.944179614369\n",
      "epoch: 80Loss: 0.944428348986\n",
      "epoch: 81Loss: 0.942585160107\n",
      "epoch: 82Loss: 0.945326950914\n",
      "epoch: 83Loss: 0.942659013912\n",
      "epoch: 84Loss: 0.942799245881\n",
      "epoch: 85Loss: 0.9409698424\n",
      "epoch: 86Loss: 0.941312853299\n",
      "epoch: 87Loss: 0.940980114696\n",
      "epoch: 88Loss: 0.941762596799\n",
      "epoch: 89Loss: 0.938485113232\n",
      "epoch: 90Loss: 0.939055920339\n",
      "epoch: 91Loss: 0.936905724742\n",
      "epoch: 92Loss: 0.93775757245\n",
      "epoch: 93Loss: 0.936388864053\n",
      "epoch: 94Loss: 0.937354660089\n",
      "epoch: 95Loss: 0.934753122281\n",
      "epoch: 96Loss: 0.935839536021\n",
      "epoch: 97Loss: 0.933321932093\n",
      "epoch: 98Loss: 0.934706754225\n",
      "epoch: 99Loss: 0.93341293038\n",
      "epoch: 100Loss: 0.934874529536\n",
      "epoch: 101Loss: 0.932743724233\n",
      "epoch: 102Loss: 0.934610005021\n",
      "epoch: 103Loss: 0.931996969716\n",
      "epoch: 104Loss: 0.931915568555\n",
      "epoch: 105Loss: 0.930570546573\n",
      "epoch: 106Loss: 0.931958305722\n",
      "epoch: 107Loss: 0.930896975515\n",
      "epoch: 108Loss: 0.93089978273\n",
      "epoch: 109Loss: 0.929475298494\n",
      "epoch: 110Loss: 0.930281226144\n",
      "epoch: 111Loss: 0.928488455773\n",
      "epoch: 112Loss: 0.929196998837\n",
      "epoch: 113Loss: 0.928183543938\n",
      "epoch: 114Loss: 0.928938113996\n",
      "epoch: 115Loss: 0.927875999238\n",
      "epoch: 116Loss: 0.927911581943\n",
      "epoch: 117Loss: 0.926859043512\n",
      "epoch: 118Loss: 0.926937997568\n",
      "epoch: 119Loss: 0.925763382058\n",
      "epoch: 120Loss: 0.926433204548\n",
      "epoch: 121Loss: 0.925278759882\n",
      "epoch: 122Loss: 0.926071928556\n",
      "epoch: 123Loss: 0.9252416003\n",
      "epoch: 124Loss: 0.925378200454\n",
      "epoch: 125Loss: 0.924522583178\n",
      "epoch: 126Loss: 0.925222852968\n",
      "epoch: 127Loss: 0.923926118321\n",
      "epoch: 128Loss: 0.923919883053\n",
      "epoch: 129Loss: 0.923246043402\n",
      "epoch: 130Loss: 0.923870767497\n",
      "epoch: 131Loss: 0.923128135007\n",
      "epoch: 132Loss: 0.923307917353\n",
      "epoch: 133Loss: 0.92272755063\n",
      "epoch: 134Loss: 0.922953957179\n",
      "epoch: 135Loss: 0.922371392494\n",
      "epoch: 136Loss: 0.922203264452\n",
      "epoch: 137Loss: 0.921412754072\n",
      "epoch: 138Loss: 0.921828902398\n",
      "epoch: 139Loss: 0.920816282419\n",
      "epoch: 140Loss: 0.921338962445\n",
      "epoch: 141Loss: 0.920125625045\n",
      "epoch: 142Loss: 0.920674442538\n",
      "epoch: 143Loss: 0.919400164578\n",
      "epoch: 144Loss: 0.920278879651\n",
      "epoch: 145Loss: 0.919469023526\n",
      "epoch: 146Loss: 0.919833711134\n",
      "epoch: 147Loss: 0.91875234443\n",
      "epoch: 148Loss: 0.919413337373\n",
      "epoch: 149Loss: 0.918648046129\n",
      "epoch: 150Loss: 0.919227205254\n",
      "epoch: 151Loss: 0.91774146318\n",
      "epoch: 152Loss: 0.918622453327\n",
      "epoch: 153Loss: 0.918303861036\n",
      "epoch: 154Loss: 0.918547121613\n",
      "epoch: 155Loss: 0.917058038654\n",
      "epoch: 156Loss: 0.917751365877\n",
      "epoch: 157Loss: 0.916649647006\n",
      "epoch: 158Loss: 0.917385776839\n",
      "epoch: 159Loss: 0.916482733971\n",
      "epoch: 160Loss: 0.917463545322\n",
      "epoch: 161Loss: 0.916570726044\n",
      "epoch: 162Loss: 0.917140059124\n",
      "epoch: 163Loss: 0.915843767079\n",
      "epoch: 164Loss: 0.916175524416\n",
      "epoch: 165Loss: 0.915585871278\n",
      "epoch: 166Loss: 0.915946898811\n",
      "epoch: 167Loss: 0.914932224425\n",
      "epoch: 168Loss: 0.915738750736\n",
      "epoch: 169Loss: 0.914800896964\n",
      "epoch: 170Loss: 0.915250685899\n",
      "epoch: 171Loss: 0.914541368276\n",
      "epoch: 172Loss: 0.915015903194\n",
      "epoch: 173Loss: 0.914230574846\n",
      "epoch: 174Loss: 0.914702400135\n",
      "epoch: 175Loss: 0.913343241689\n",
      "epoch: 176Loss: 0.913985514691\n",
      "epoch: 177Loss: 0.913024301231\n",
      "epoch: 178Loss: 0.913565632911\n",
      "epoch: 179Loss: 0.912152637954\n",
      "epoch: 180Loss: 0.912807466213\n",
      "epoch: 181Loss: 0.911920779606\n",
      "epoch: 182Loss: 0.91217022389\n",
      "epoch: 183Loss: 0.910751617828\n",
      "epoch: 184Loss: 0.911628448096\n",
      "epoch: 185Loss: 0.910783972367\n",
      "epoch: 186Loss: 0.910986428844\n",
      "epoch: 187Loss: 0.909771409542\n",
      "epoch: 188Loss: 0.909832196329\n",
      "epoch: 189Loss: 0.908391065433\n",
      "epoch: 190Loss: 0.909110937121\n",
      "epoch: 191Loss: 0.908044301746\n",
      "epoch: 192Loss: 0.908086347207\n",
      "epoch: 193Loss: 0.907453784999\n",
      "epoch: 194Loss: 0.90858869662\n",
      "epoch: 195Loss: 0.905747039694\n",
      "epoch: 196Loss: 0.906257099697\n",
      "epoch: 197Loss: 0.903943286819\n",
      "epoch: 198Loss: 0.904391792138\n",
      "epoch: 199Loss: 0.902201786535\n",
      "epoch: 200Loss: 0.902534059107\n"
     ]
    }
   ],
   "source": [
    "#Training SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "    train_loss = 0.\n",
    "    counter = 0\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(train_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/ float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            counter += 1\n",
    "            optimizer.step()\n",
    "    print('epoch: '+ str(epoch) + ' Loss: ' + str(train_loss/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.961124987614\n"
     ]
    }
   ],
   "source": [
    "#testing SAE\n",
    "test_loss = 0.\n",
    "counter = 0\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(train_set[id_user]).unsqueeze(0) #visible observations remain same\n",
    "    target = Variable(test_set[id_user])#actual ratings in test set\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/ float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "        counter += 1\n",
    "print('Test Loss: ' + str(test_loss/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
